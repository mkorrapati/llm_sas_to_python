{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAS Function to Python using GenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scikit-learn\n",
    "%pip install openai\n",
    "# Install required modules and set required tokens\n",
    "%pip install openai python-dotenv\n",
    "%pip install requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the .env file in a Jupyter notebook:\n",
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "import os\n",
    "import json\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup proxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/muralikorrapati/pythonPractice/MLearning/llm_sas_to_python/.venv/lib/python3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from requests.auth import HTTPProxyAuth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure your proxy\n",
    "proxy = \"http://95.164.128.235:9943\"\n",
    "proxy_auth = HTTPProxyAuth('M0k0EN', 'ZSNkKy')\n",
    "\n",
    "# Configure a session to use the proxy\n",
    "session = requests.Session()\n",
    "session.proxies = {'http': proxy, 'https': proxy}\n",
    "session.auth = proxy_auth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.environ.get('OPENAI_API_KEY')\n",
    "# openai.organization = os.environ.get('OPENAI_ORG_ID')\n",
    "if os.environ.get('use_proxy'):\n",
    "    openai.session = session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://sebscholl.medium.com/using-chatgpt-in-jupyter-notebooks-c6144fef79d8\n",
    "class ChatSession:\n",
    "    \"\"\"\n",
    "    Class for running a chat session. \n",
    "    \n",
    "    Create and use a new chat by running:\n",
    "        chat = ChatSession('new-chat-name', 'system-role-message')\n",
    "\n",
    "    Load an existing chat by running:\n",
    "        chat = ChatSession('existing-chat-name')\n",
    "    \"\"\"\n",
    "    model = 'gpt-3.5-turbo-16k-0613'\n",
    "    \n",
    "    def __init__(self, chat_name, system_role=None):\n",
    "        self.name = chat_name\n",
    "        self.path = f\"./chats/{chat_name}.json\"\n",
    "\n",
    "        if system_role is not None:\n",
    "            with open(self.path, \"w\") as outfile:\n",
    "                outfile.write(json.dumps([system_role], indent=4))\n",
    "            print(f\"Chat {self.path} created ðŸ’¬\")\n",
    "        else:\n",
    "            print(f\"Active chat set to {self.name} ðŸ’¬\")\n",
    "            \n",
    "    def chat(self, message):\n",
    "        # Retrieve chat history\n",
    "        with open(self.path) as openchat:\n",
    "            chat_history = json.load(openchat)\n",
    "        # Update chat history before completion\n",
    "        chat_history.append({'role': 'user', 'content': message})\n",
    "        # Run completeion\n",
    "        chat_completion = openai.chat.completions.create(model=self.model, messages=chat_history)\n",
    "        \n",
    "        # Save message to chat\n",
    "        chat_history.append({'role': 'system', 'content':chat_completion.choices[0].message.content})\n",
    "        # Handle length finish reason\n",
    "        if chat_completion.choices[0].finish_reason == 'length':\n",
    "            chat_completion = openai.chat.completions.create(model=self.model, messages=chat_history)\n",
    "            \n",
    "        # Save the chat history\n",
    "        with open(self.path, \"w\") as outfile:\n",
    "            json.dump(chat_history, outfile)\n",
    "        # Print new dialog.\n",
    "        for msg in chat_history[-2:]:\n",
    "            print(f\"ROLE: {msg['role']}\")\n",
    "            print(\"------------------------\")\n",
    "            print(f\"{msg['content']}\")\n",
    "            # for line in msg[\"content\"]:\n",
    "            #     print(f\"\\t{line}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start chat session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat ./chats/philosophical-chat.json created ðŸ’¬\n"
     ]
    }
   ],
   "source": [
    "c1 = ChatSession(\"philosophical-chat\", { \"role\": \"system\", \"content\": \"You are an AI assistant that converts SAS code to Python.\" }) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROLE: user\n",
      "\n",
      "/* Assign some macro variables */\n",
      "%let var1 = test;\n",
      "%let id = 39;\n",
      "\n",
      "/* Print to log */\n",
      "%put var1 = &var1.;\n",
      "%put id = &id.;\n",
      "/* Use the macro variables in a data step to create table */\n",
      "data test;\n",
      "       m&id. = 333;\n",
      "       v = \"&var1.\";\n",
      "run;\n",
      "\n",
      "ROLE: system\n",
      "# Assign some macro variables\n",
      "var1 = \"test\"\n",
      "id = 39\n",
      "\n",
      "# Print to log\n",
      "print(\"var1 =\", var1)\n",
      "print(\"id =\", id)\n",
      "\n",
      "# Use the macro variables in a data step to create table\n",
      "test = pd.DataFrame({\n",
      "    \"m\" + str(id): [333],\n",
      "    \"v\": [var1]\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "c1.chat(\"\"\"\n",
    "/* Assign some macro variables */\n",
    "%let var1 = test;\n",
    "%let id = 39;\n",
    "\n",
    "/* Print to log */\n",
    "%put var1 = &var1.;\n",
    "%put id = &id.;\n",
    "/* Use the macro variables in a data step to create table */\n",
    "data test;\n",
    "       m&id. = 333;\n",
    "       v = \"&var1.\";\n",
    "run;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
